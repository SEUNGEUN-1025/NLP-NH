# NLP - AIì•¼, ì§„ì§œ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ì¤˜!

* ëŒ€íšŒ: [2020 NHíˆ¬ìì¦ê¶Œ ë¹…ë°ì´í„° ê²½ì§„ëŒ€íšŒ](https://dacon.io/competitions/official/235658/overview/description/)
* í™œë™ê¸°ê°„: 20/11/23 ~ 20/12/31
* ìì„¸í•œ ë‚´ìš©ì€ [ë³´ê³ ì„œ](https://github.com/SEUNGEUN-1025/NLP-NH/blob/2d49c8c421760b1fdad3583d74f5ee47864fc6d0/%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C_%EC%96%BC%EA%B7%B8%EB%A0%88%EC%9D%B4.pdf) ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.

## ğŸ“° ë¶„ì„ ë¬¸ì œ
- ì§„ì§œ ë‰´ìŠ¤ì™€ ê°€ì§œ ë‰´ìŠ¤ë¥¼ êµ¬ë³„í•˜ëŠ” ë¹ ë¥´ê³  ì •í™•í•œ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ

## ğŸ“° ë¶„ì„ ê³¼ì •
### 1. ë¶„ì„ ë°ì´í„°
- 2020.01.01 ~ 2020.06.30ê¹Œì§€ì˜ ì‚¬íšŒ, ê²½ì œ ë¶„ì•¼ ë‰´ìŠ¤ Data
- ê¸°ì‚¬ì˜ titleê³¼ contentë¥¼ í•©í•œ ë‚´ìš©ì„ Input Dataë¡œ ì‚¬ìš©í•¨ 
### 2. Tokenizer
- ë¶„ì„ì— í•„ìš”í•œ ì ì ˆí•œ ë‹¨ì–´ë“¤ì„ ê°€ì§„ Vocabì„ ì´ìš©í•œ Tokenizerë¥¼ ì´ìš©í•´ì•¼ í•¨
- ë‹¨ì–´ ìˆ˜ê°€ ë§ì€ ëª¨ë¸([KRBERT(snunlp)](https://github.com/snunlp/KR-BERT))ì˜ tokenizerë¡œ ë‚˜ëˆˆ [ë‹¨ì–´ë“¤](https://github.com/SEUNGEUN-1025/NLP-NH/blob/2d49c8c421760b1fdad3583d74f5ee47864fc6d0/plus%201500%20vocab.csv)ì˜ ìƒìœ„ 500ê°œë¥¼ ì¶”ì¶œí•˜ì—¬ Tokenizerì— ì¶”ê°€
### 3. BERT
- [SKT Brain](https://github.com/SKTBrain/KoBERT)íŒ€ì´ ê°œë°œí•œ KoBERTë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ì„ ìˆ˜í–‰í•¨
### 4. Modeling & Training
- KoBERTì— Dense Layer ì¶”ê°€í•˜ì—¬ ì„±ëŠ¥ì„ ë†’ì´ë ¤ í•¨
- í•™ìŠµ í™˜ê²½

|í•­ëª©|ë‚´ìš©|
|------|---|
|Parameters|92,244,481|
|Batch size|32|
|Training Steps|3340|
|Epochs|13|
|Time|20min/epoch|

## ğŸ“° ë¶„ì„ ê²°ê³¼
- Validation Loss = 0.0329 , Validation Accuracy = 0.9936
<img width="1041" alt="loss_acc" src="https://user-images.githubusercontent.com/82666244/116254883-b05c8900-a7ac-11eb-8cb0-0450cdd271c3.png">

- ìµœì¢… Test Accuracy = __0.98214__

